group_by(island) %>%
print(summarise(count = n())) %>%
ungroup() %>%
head(3)
penguins %>%
filter(island %in%  c("Biscoe", "Torgersen")) %>%
paste(nrow()) %>%
group_by(island) %>%
print(summarise(count = n())) %>%
ungroup() %>%
head(3)
penguins %>%
filter(island %in%  c("Biscoe", "Torgersen")) %>%
print(nrow()) %>%
group_by(island) %>%
print(summarise(count = n())) %>%
ungroup() %>%
head(3)
knitr::opts_chunk$set(echo = TRUE)
dat <- read.csv("data-table-B7.csv")
#head(dat)
View(dat)
lm(y~., dat)
summary(lm(y~., dat))
summary(lm(y~x1 + x2 + x3 + x5, dat))
summary(lm(y~x1 + x2  + x5, dat))
summary(lm(y~ x2  + x5, dat))
setwd("~/Desktop/Regressions/Regression Models/Linear/Logistic")
knitr::opts_chunk$set(echo = TRUE)
data <- read.csv("Social_Network_Ads.csv")
View(data)
library(tidyverse)
library(caTools)
data <- data[ , 2:3]
data <- data[ , 3:5]
data <- read.csv("Social_Network_Ads.csv")
data <- data[ , 3:5]
set.seed(1)
set.seed(1)
split <- sample.split(data$Purchased, SplitRatio = .75)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
View(training)
View(test)
dim(training)
dim(test)
dim(training)
dim(test)
training[ , 1:2] <- scale(training[ , 1:2])
test[ , 1:2] <- scale(test[ , 1:2])
# Fitting Logistic Regression to the training set
classifier <- glm(Purchased~., training)
# Fitting Logistic Regression to the training set
classifier <- glm(Purchased~., data = training)
# Fitting Logistic Regression to the training set
classifier <- glm(Purchased~., family = binomial, data = training)
# Fitting Logistic Regression to the training set
classifier <- glm(Purchased~.,
family = binomial,
data = training)
summary(classifier)
anova(classifier)
summary(classifier)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-3])
prob_pred
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
y_pred
# Making the Confusion Matrix
cm <- table(test[ , 3], y_pred)
cm
library(ElemStatLearn)
install.packages(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
set = training
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caTools)
#install.packages("ElemStatLearn")
library(ElemStatLearn)
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# Visualizing the data
set = training
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Training set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
set = test
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
grid_set = expand.grid(X1, X2)
colnames(grid_set) = c('Age', 'EstimatedSalary')
prob_set = predict(classifier, type = 'response', newdata = grid_set)
y_grid = ifelse(prob_set > 0.5, 1, 0)
plot(set[, -3],
main = 'Logistic Regression (Test set)',
xlab = 'Age', ylab = 'Estimated Salary',
xlim = range(X1), ylim = range(X2))
contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
setwd("~/Desktop/Regressions/Breast-Cancer-Detection-Using-Logistic-Regression")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caTools)
#install.packages("ElemStatLearn")
library(ElemStatLearn)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caTools)
#install.packages("ElemStatLearn")
#library(ElemStatLearn)
# Reda the data
data <- read.csv("breast_cancer.csv")
View(data)
# Reda the data
data <- read.csv("../data/breast_cancer.csv")
# Reda the data
data <- read.csv("./data/breast_cancer.csv")
View(data)
View(data)
data <- read.csv("./data/breast_cancer.csv")
data <- data[ , :10]
data <- data[ , 0:10]
data <- data[ , 2:10]
data <- data[ , -1]
# Reda the data
data <- read.csv("./data/breast_cancer.csv")
data <- data[ , -1]
# Splitting the data
set.seed(1)
split <- sample.split(data$Purchased, SplitRatio = .8)
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
library(officedown)
install.packages("officedown")
library(officedown)
unlink('BreastCancerDetection_cache', recursive = TRUE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caTools)
x <- data[ , -10]
y <- data[ , 10]
data <- read.csv("./data/breast_cancer.csv")
x <- data[ , -10]
y <- data[ , 10]
View(x)
View(y)
x <- ifelse(x == 4, 1, 0)
# Reda the data
data <- read.csv("./data/breast_cancer.csv")
x <- data[ , -10]
y <- data[ , 10]
y <- ifelse(x == 4, 1, 0)
# Reda the data
data <- read.csv("./data/breast_cancer.csv")
x <- data[ , -10]
y <- data[ , 10]
y <- data$Class
# Reda the data
data <- read.csv("./data/breast_cancer.csv")
x <- data[ , -11]
y <- data$Class
y <- ifelse(y$V1 == 4, 1, 0)
y <- ifelse(y == 4, 1, 0)
sum(is.na(data))
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(Class~.,
family = binomial,
data = training)
# Fitting Logistic Regression to the training set
classifier <- glm(y~x,
family = binomial)
x <- data[ , -c(1,11)] # Regressors / Independent Variables
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(y~x,
family = binomial)
set.seed(1)
split <- sample.split(x, SplitRatio = .8)
training_x <- subset(data, split == TRUE)
test_x <- subset(data, split == FALSE)
set.seed(1)
split <- sample.split(x, SplitRatio = .8)
training_x <- subset(data, split == TRUE)
test_x <- subset(data, split == FALSE)
split <- sample.split(y, SplitRatio = .8)
training_y <- subset(data, split == TRUE)
test_y <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(training_y~training_x,
family = binomial)
classifier <- glm(training_y~training_x,
family = binomial)
# Fitting Logistic Regression to the training set
classifier <- glm(training_x~training_y,
family = binomial)
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
data$Class <- ifelse(Class == 4, 1, 0)
data <- ifelse(Class == 4, 1, 0)
data$Class <- ifelse(data$Class == 4, 1, 0)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
set.seed(1)
split <- sample.split(x, SplitRatio = .8)
training_x <- subset(data, split == TRUE)
test_x <- subset(data, split == FALSE)
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-3])
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 3], y_pred)
set = training
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# Splitting the data
set.seed(1)
split <- sample.split(x, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 3], y_pred)
# Visualizing the data
set = training
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
cm
# Splitting the data
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
# Visualizing the data
set = training
X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
data$Class <- ifelse(data$Class == 4, 1, 0)
str(data)
# Read the data
data <- read.csv("./data/breast_cancer.csv")
sum(is.na(data)) # No missing values
data$Class <- ifelse(data$Class == 4, 1, 0)
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
summary(classifier)
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
summary(classifier)
summary(classifier)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ Sample.code.number + Clump.Thickness + Uniformity.of.Cell.Size + Uniformity.of.Cell.Shape + Marginal.Adhesion + Single.Epithelial.Cell.Size + Bare.Nuclei + Bland.Chromatin + Normal.Nucleoli + Mitoses,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
summary(classifier)
classifier <- glm(Class ~ Clump.Thickness + Uniformity.of.Cell.Shape + Marginal.Adhesion + Single.Epithelial.Cell.Size + Bare.Nuclei + Bland.Chromatin + Normal.Nucleoli + Mitoses,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
summary(classifier)
classifier <- glm(Class ~ Clump.Thickness + Uniformity.of.Cell.Shape + Marginal.Adhesion +  Bare.Nuclei + Bland.Chromatin + Normal.Nucleoli + Mitoses,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
summary(classifier)
classifier <- glm(Class ~ Clump.Thickness + Uniformity.of.Cell.Shape + Marginal.Adhesion +  Bare.Nuclei + Bland.Chromatin + Normal.Nucleoli,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
classifier <- glm(Class ~ Marginal.Adhesion +  Bare.Nuclei + Bland.Chromatin + Normal.Nucleoli,
family = binomial,
data = data)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
summary(classifier)
# Splitting the data
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
summary(classifier)
# Feature Scaling
training[ , 1:10] <- scale(training[ , 1:10])
test[ , 1:10] <- scale(test[ , 1:10])
# Splitting the data
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Feature Scaling
training[ , 1:10] <- scale(training[ , 1:10])
test[ , 1:10] <- scale(test[ , 1:10])
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
summary(classifier)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
# Splitting the data
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Feature Scaling
training[ , 1:10] <- scale(training[ , 1:10])
test[ , 1:10] <- scale(test[ , 1:10])
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
summary(classifier)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
cm
View(training)
View(test)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
summary(classifier)
cm
# Splitting the data
set.seed(1)
split <- sample.split(data$Class, SplitRatio = .8)
training <- subset(data, split == TRUE)
test <- subset(data, split == FALSE)
# Fitting Logistic Regression to the training set
classifier <- glm(Class ~ . ,
family = binomial,
data = data)
summary(classifier)
# Predicting the test set results
prob_pred <- predict(classifier,
type = 'response',
newdata = test[-11])
y_pred <- ifelse(prob_pred > 0.5, 1, 0)
# Making the Confusion Matrix
cm <- table(test[ , 11], y_pred)
# Model Accuracy
model_accuracy <- (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[1,2] + cm[2,1] + cm[2,2])
paste("Assuming 50% probability as cutoff, model accuracy is ", model_accuracy)
sum(is.na(data)) # No missing values
data$Class <- ifelse(data$Class == 4, 1, 0)
